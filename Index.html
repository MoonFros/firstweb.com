<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Looksly AI</title>
<style>
    body { font-family: Arial, sans-serif; background: #f0f2f5; margin: 0; padding: 0; display: flex; flex-direction: column; height: 100vh; }
    #chat { flex: 1; overflow-y: auto; padding: 20px; background: #fff; }
    .message { margin: 10px 0; }
    .user { text-align: right; color: #333; }
    .bot { text-align: left; color: #007bff; }
    #input-area { display: flex; padding: 10px; background: #ddd; }
    #input-area input, #input-area button { padding: 10px; font-size: 1rem; }
    #input-area input[type="file"] { display: none; }
    button { cursor: pointer; }
</style>
</head>
<body>

<div id="chat"></div>

<div id="input-area">
    <input type="text" id="text-input" placeholder="Type your message..." />
    <button id="send-btn">Send</button>
    <button id="image-btn">ðŸ“·</button>
    <input type="file" id="image-input" accept="image/*" />
    <button id="audio-btn">ðŸŽ¤</button>
    <button id="tts-btn">ðŸ”Š</button>
</div>

<script>
const chat = document.getElementById('chat');
const textInput = document.getElementById('text-input');
const sendBtn = document.getElementById('send-btn');
const imageBtn = document.getElementById('image-btn');
const imageInput = document.getElementById('image-input');
const audioBtn = document.getElementById('audio-btn');
const ttsBtn = document.getElementById('tts-btn');

// Utility to append messages
function appendMessage(sender, text) {
    const div = document.createElement('div');
    div.className = 'message ' + sender;
    div.textContent = text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
}

// ----- Text Chat -----
async function sendText() {
    const text = textInput.value.trim();
    if (!text) return;
    appendMessage('user', text);
    textInput.value = '';

    // Call OpenAI API (replace YOUR_API_KEY with your actual key)
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer YOUR_API_KEY'
        },
        body: JSON.stringify({
            model: 'gpt-4o-mini',
            messages: [{ role: 'user', content: text }]
        })
    });
    const data = await response.json();
    const reply = data.choices[0].message.content;
    appendMessage('bot', reply);
}

sendBtn.addEventListener('click', sendText);
textInput.addEventListener('keypress', e => { if (e.key === 'Enter') sendText(); });

// ----- Image Upload -----
imageBtn.addEventListener('click', () => imageInput.click());
imageInput.addEventListener('change', async () => {
    const file = imageInput.files[0];
    if (!file) return;
    appendMessage('user', `[Image: ${file.name}]`);
    const reader = new FileReader();
    reader.onload = async () => {
        const base64 = reader.result.split(',')[1];
        // Call OpenAI Image Generation API
        const res = await fetch('https://api.openai.com/v1/images/generations', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer YOUR_API_KEY'
            },
            body: JSON.stringify({
                prompt: 'Generate a creative variation of this image',
                n: 1,
                size: '512x512'
            })
        });
        const imgData = await res.json();
        const imgUrl = imgData.data[0].url;
        appendMessage('bot', `Generated Image: ${imgUrl}`);
    };
    reader.readAsDataURL(file);
});

// ----- Audio Input -----
let recognition;
if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognition = new SpeechRecognition();
    recognition.lang = 'en-US';
    recognition.interimResults = false;

    recognition.onresult = e => {
        const transcript = e.results[0][0].transcript;
        appendMessage('user', transcript);
        textInput.value = transcript;
        sendText();
    };
}

audioBtn.addEventListener('click', () => {
    if (recognition) recognition.start();
    else alert('Speech Recognition not supported in this browser.');
});

// ----- Text-to-Speech -----
ttsBtn.addEventListener('click', () => {
    const lastBotMessage = [...chat.getElementsByClassName('bot')].pop();
    if (!lastBotMessage) return;
    const utter = new SpeechSynthesisUtterance(lastBotMessage.textContent);
    speechSynthesis.speak(utter);
});
</script>
</body>
</html>
